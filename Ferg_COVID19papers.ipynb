{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analysis of COVID-19 Research Papers\n",
    "## Robyn Ferg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "COVID-19 is a highly infectious, novel respiratory disease. Originating in Wuhan, China in late 2019, COVID-19 has quickly become a worldwide pandemic. Being a novel virus, much is unknown about COVID-19. Numerous academic studies have been and continue to be performed on various aspects of the virus, from vaccine research to social implications. Inspecting each of these studies individually is and onerous task. Instead, we rely on natural language processing techniques to extract meaningful information from a corpus of research papers relating to COVID-19.\n",
    "\n",
    "In this report we extract research paper information from an online repository, cluster papers into topics, provide words and papers automatically generated to represent each of those topics, describe an algorithm for providing a summary of each paper, and describe how we could extract papers that may provide breakthroughs in treatment and prevention of COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Our data comes from an online repository of COVID-19 research papers compiled by the MIDAS Network Coordination Center. \n",
    "\n",
    "We load in the xml file and convert to a data frame. We only consider the title of the article, journal name, and abstract. Other variables exist in the xml files, but many have high levels of missingness or are presumably unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/midas-network/COVID-19/master/documents/mendeley_library_files/xml_files/mendeley_document_library_2020-03-25.xml'\n",
    "document = requests.get(url)\n",
    "soup = BeautifulSoup(document.content, 'lxml-xml')\n",
    "\n",
    "first_child = soup.find('xml')\n",
    "second_child = first_child.find('records')\n",
    "\n",
    "# create pandas data frame\n",
    "data = []\n",
    "for paper in second_child.contents:\n",
    "\ttitle = paper.find('titles').find('title').text\n",
    "\t# journal title\n",
    "\tperiodical = paper.find('periodical')\n",
    "\tif len(periodical)==0:\n",
    "\t\tjournal = ' '\n",
    "\telse:\n",
    "\t\tjournal = periodical.find('full-title').text\n",
    "\t# abstract\n",
    "\tabstract = paper.find('abstract')\n",
    "\tif abstract is None:\n",
    "\t\tabstract = ' '\n",
    "\telse:\n",
    "\t\tabstract = abstract.text\n",
    "\t# append to data\n",
    "\tdata.append([title, journal, abstract])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['title', 'journal', 'abstract_raw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we perform any analyses on the data, we first pre-process the data. The following code pre-process the text of the abstracts in the following ways:\n",
    "* Removes punctuation\n",
    "* Removes stopwords\n",
    "* Stems words, i.e. removing suffixes such as -s, -ing, etc.\n",
    "* Some abstracts contain competing interest statments, funding statements, etc. that are unimportant to the content of the research being presented. We remove those statements.\n",
    "* Remove formatting symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def removeStmts(abstract):\n",
    "\tif 'Competing Interest Statement' in abstract:\n",
    "\t\tabstract = abstract[0:abstract.find('Competing Interest Statement')]\n",
    "\treturn(abstract)\n",
    "\n",
    "def preProcessingFcn(text, removeWords=list(), stem=True, removeURL=True, removeStopwords=True, \n",
    "\tremoveNumbers=False, removeHashtags=True, removeAt=True, removePunctuation=True):\n",
    "\tps = PorterStemmer()\n",
    "\ttext = text.lower()\n",
    "\ttext = re.sub(r\"\\\\n\", \" \", text)\n",
    "\ttext = re.sub(r\"&amp\", \" \", text)\n",
    "\tif removeHashtags==True:\n",
    "\t\ttext = text.replace('#', ' ')\n",
    "\tif removeNumbers==True:\n",
    "\t\ttext=  ''.join(i for i in text if not i.isdigit())\n",
    "\tif removePunctuation==True:\n",
    "\t\ttext = re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", text)\n",
    "\tif removeStopwords==True:\n",
    "\t\ttext = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "\tif len(removeWords)>0:\n",
    "\t\ttext = ' '.join([word for word in text.split() if word not in removeWords])\n",
    "\tif stem==True:\n",
    "\t\ttext = ' '.join([ps.stem(word) for word in text.split()])\n",
    "\treturn text\n",
    "\n",
    "removeWords = ['<p>', '</p>', '<bold>', '</bold>']\n",
    "\n",
    "df['abstract_noStmt'] = [removeStmts(abst) for abst in df['abstract_raw']]\n",
    "df['abstract'] = [preProcessingFcn(abst, removeWords) for abst in df['abstract_noStmt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the papers have no abstract. We create a word-document matrix from the set of abstracts. That is, we create a matrix $w$ such that $w_{ij}$ gives the number of times word $j$ appears in document $i$. By doing this, we are using a bag-of-words model, where the ordering of words within a text are not taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='unicode')\n",
    "textsNew = vectorizer.fit_transform(df['abstract'])\n",
    "wOriginal = textsNew.toarray()\n",
    "textsIndex = list(np.where(np.sum(wOriginal, axis=1)>0)[0])\n",
    "w = wOriginal[textsIndex,:]\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "The goal of this section is to automatically group similar papers together. To accomplish this goal, we make use of Latent Dirichlet Allocation, Latent Semantic Analysis, and clustering, each described below.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is perhaps to most common method of modeling topics in a corpus of texts (Blei, Ng, Jordan 2003). LDA is a hierarchical Bayesian model that assumes each document in a corpus of texts is comprised of a distribution of topics, where a topic is defined as a probability distribution over words. This assumption of each text containing multiple topics is appropriate for our context: paper abstracts may contain, for example, background information, biological breakthrough, and societal impacts. More formally, let $M$ denote the number of documents and $N_i$ denote the number of words in document $i$. Document $i$ has topic distribution $\\theta_i$, where $\\theta_i\\sim Dirichlet(\\alpha)$. Each word $w_{ij}$ in document $i$ belongs to topic $z_{ij}\\sim\\theta_i$, and $w_{ij}\\sim Dirichlet(\\beta)$, where $\\beta$ is the prior on the per-topic word distribution. In this model, only the words $w$ are known, the remaining variables are latent. \n",
    "\n",
    "For a chosen number of $k$ topics, the LDA algorithm assigns each text a $k$-dimensional probability distribution across the $k$ topics. If two texts are in some sense similar, the probability distribution across topics may also be similar. Therefore, we calculate the distance between two texts as the Euclidean distance between the $k$-dimensional probability distributions for the two texts.\n",
    "\n",
    "Alternatively, we can calculate distance between texts using Latent Semantic Analysis (LSA) (Dumais et al. 1988). LSA, like LDA, first creates a word-document matrix $w$, where each row of $w$ refers to a single document and each column refers to a single word, where $w_{ij}$ is the number of times word $j$ appears in document $i$. $w$ is a large and often sparse matrix. Dimension reduction, such as singluar value decomposition, is then applied to $w$. The distance beteen two texts $X$ and $Y$ is calculated using cosine similarity: $$dist(X, Y) = 1-\\frac{X \\cdot Y}{||X||\\times||Y||}$$\n",
    "\n",
    "Note that both LDA and LSA are bag-of-words methods, so the ordering of words within a document are not taken into account.\n",
    "\n",
    "Both distances, as calculate by LDA and LSA, may carry important information. Furthermore, human inspection of words most highly expressed in each topic in LDA and most heaving weighted words in the components using LSA suggest that both methods appear to work fairly well. To hopefully capture advantages of both methods, we create two distance matrices, one for LDA and one for LSA, normalize so all distances are between 0 and 1, and average the two normalized distance matrices.\n",
    "\n",
    "After obtaining the final distances between each abstract in the data set of papers, we apply a clustering method to sort each paper into just one latent topic. Since we only have distances between documents as opposed to points in space, we apply k-medoids clustering to the distance matrix.\n",
    "\n",
    "The following chunks of code calculate distances between the abstracts using LDA, calculate the distances between abstracts using LSA, average the two distance matrices, and apply k-medoids. For LDA we use 7 clusters, having the maximum coherence score (code omitted from this write-up). For LSA, we reduce $w$ to 14-dimensions. Following LDA, we perform k-medoids clustering using 7 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robyn\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "numberTopics = 7\n",
    "lda = LDA(n_components=numberTopics, random_state=0)\n",
    "ldaFit = lda.fit(w)\n",
    "topicDistributions = lda.transform(w)\n",
    "distsLDA = distance.cdist(topicDistributions, topicDistributions, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "random.seed(234)\n",
    "\n",
    "n_components = 14\n",
    "svd_model = TruncatedSVD(n_components=n_components, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(w)\n",
    "wDimReduced = svd_model.fit_transform(w)\n",
    "distsLSA = 1-cosine_similarity(wDimReduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Distance Matrix\n",
    "distsLDA_normalized = distsLDA/np.max(distsLDA)\n",
    "distsLSA_normalized = distsLSA/np.max(distsLSA)\n",
    "\n",
    "meanDists = (distsLDA_normalized + distsLSA_normalized)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering: k-medoids\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "\n",
    "random.seed(345)\n",
    "\n",
    "numberTopics = 7\n",
    "kmedoids = kmedoids(meanDists, range(numberTopics), data_type='distance_matrix')\n",
    "kmedoids.process()\n",
    "clusters = kmedoids.get_clusters()\n",
    "medoids = kmedoids.get_medoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of number of papers in each cluster is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198, 61, 89, 196, 59, 105, 142]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(clusters[i]) for i in range(numberTopics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centers of each resulting cluster can be considered to best represent the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0.0\n",
      "Title:\n",
      "Estimation of the Time-Varying Reproduction Number of COVID-19 Outbreak in China\n",
      "In:\n",
      "medRxiv\n",
      "Abstract:\n",
      "Background: The 2019-nCoV outbreak in Wuhan, China has attracted world-wide attention. As of February 11, 2020, a total of 44730 cases of novel coronavirus-infected pneumonia associated with COVID-19 were confirmed by the National Health Commission of China. Methods: Three approaches, namely Poisson likelihood-based method (ML), exponential growth rate-based method (EGR) and stochastic Susceptible-Infected-Removed dynamic model-based method (SIR), were implemented to estimate the basic and controlled reproduction numbers. Results: A total of 71 chains of transmission together with dates of symptoms onset and 67 dates of infections were identified among 5405 confirmed cases outside Hubei as reported by February 2, 2020. Based on this information, we find the serial interval having an average of 4.41 days with a standard deviation of 3.17 days and the infectious period having an average of 10.91 days with a standard deviation of 3.95 days. Conclusions: The controlled reproduction number is declining. It is lower than one in most regions of China, but is still larger than one in Hubei Province. Sustained efforts are needed to further reduce the Rc to below one in order to end the current epidemic. ### \n",
      " \n",
      "Cluster 1.0\n",
      "Title:\n",
      "Development of CRISPR as a prophylactic strategy to combat novel coronavirus and influenza\n",
      "In:\n",
      "bioRxiv\n",
      "Abstract:\n",
      "The outbreak of the coronavirus disease 2019 (COVID-19), caused by the Severe Acute Respiratory Syndrome coronavirus 2 (SARS-CoV-2), has infected more than 100,000 people worldwide with over 3,000 deaths since December 2019. There is no cure for COVID-19 and the vaccine development is estimated to require 12-18 months. Here we demonstrate a CRISPR-Cas13-based strategy, PAC-MAN (Prophylactic Antiviral CRISPR in huMAN cells), for viral inhibition that can effectively degrade SARS-CoV-2 sequences and live influenza A virus (IAV) genome in human lung epithelial cells. We designed and screened a group of CRISPR RNAs (crRNAs) targeting conserved viral regions and identified functional crRNAs for cleaving SARS-CoV-2. The approach is effective in reducing respiratory cell viral replication for H1N1 IAV. Our bioinformatic analysis showed a group of only six crRNAs can target more than 90% of all coronaviruses. The PAC-MAN approach is potentially a rapidly implementable pan-coronavirus strategy to deal with emerging pandemic strains.\n",
      " \n",
      "Cluster 2.0\n",
      "Title:\n",
      "The novel coronavirus 2019 (2019-nCoV) uses the SARS-coronavirus receptor ACE2 and the cellular protease TMPRSS2 for entry into target cells\n",
      "In:\n",
      "bioRxiv\n",
      "Abstract:\n",
      "The emergence of a novel, highly pathogenic coronavirus, 2019-nCoV, in China, and its rapid national and international spread pose a global health emergency. Coronaviruses use their spike proteins to select and enter target cells and insights into nCoV-2019 spike (S)-driven entry might facilitate assessment of pandemic potential and reveal therapeutic targets. Here, we demonstrate that 2019-nCoV-S uses the SARS-coronavirus receptor, ACE2, for entry and the cellular protease TMPRSS2 for 2019-nCoV-S priming. A TMPRSS2 inhibitor blocked entry and might constitute a treatment option. Finally, we show that the serum form a convalescent SARS patient neutralized 2019-nCoV-S-driven entry. Our results reveal important commonalities between 2019-nCoV and SARS-coronavirus infection, which might translate into similar transmissibility and disease pathogenesis. Moreover, they identify a target for antiviral intervention.One sentence summary The novel 2019 coronavirus and the SARS-coronavirus share central biological properties which can guide risk assessment and intervention.\n",
      " \n",
      "Cluster 3.0\n",
      "Title:\n",
      "The spatiotemporal estimation of the dynamic risk and the international transmission of 2019 Novel Coronavirus (COVID-19) outbreak: A global perspective\n",
      "In:\n",
      "medRxiv\n",
      "Abstract:\n",
      "An ongoing novel coronavirus SARS-CoV-2 pneumonia infection outbreak called COVID-19 started in Wuhan, Hubei Province, China, in December 2019. It both spread rapidly to all provinces in China and started spreading around the world quickly through international human movement from January 2020. Currently, the spatiotemporal epidemic transmission patterns, prediction models, and possible risk analysis for the future are insufficient for COVID-19 but we urgently need relevant information, particularly from the global perspective. We have developed a novel two-stage simulation model to simulate the spatiotemporal changes in the number of COVID-19 cases and estimate the future worldwide risk. Based on the connectivity of countries to China and the country's medical and epidemic prevention capabilities, different scenarios are generated to analyze the possible transmission throughout the world and use this information to evaluate each country's vulnerability to and the dynamic risk of COVID-19. Countries' vulnerability to the COVID-19 outbreak from China is calculated for 63 countries around the world. Taiwan, South Korea, Hong Kong, and Japan are the most vulnerable areas. The relationship between each country's vulnerability and days before the first imported case occurred shows a very high exponential decrease. The cumulative number of cases in each country also has a linear relationship with vulnerability, which can compare and quantify the initial epidemic prevention capabilities to various countries' management strategies. In total, 1,000 simulation results of future cases around the world are generated for the spatiotemporal risk assessment. According to the simulation results of this study, if there is no specific medicine for it, it will likely form a global pandemic. This method can be used as a preliminary risk assessment of the spatiotemporal spread for a new global epidemic. * Note: This study was completed on February 15, 2020.\n",
      "\n",
      "### \n",
      " \n",
      "Cluster 4.0\n",
      "Title:\n",
      "Crystal structure of SARS-CoV-2 nucleocapsid protein RNA binding domain reveals potential unique drug targeting sites\n",
      "In:\n",
      "bioRxiv\n",
      "Abstract:\n",
      "The outbreak of coronavirus disease (COVID-19) in China caused by SARS-CoV-2 virus continually lead to worldwide human infections and deaths. It is currently no specific viral protein targeted therapeutics yet. Viral nucleocapsid protein is a potential antiviral drug target, serving multiple critical functions during the viral life cycle. However, the structural information of SARS-CoV-2 nucleocapsid protein is yet to be clear. Herein, we have determined the crystal structure of the N-terminal RNA binding domain of SARS-CoV-2 nucleocapsid protein. Although overall structure is similar with other reported coronavirus nucleocapsid protein N-terminal domain, the surface electrostatic potential characteristics between them are distinct. Further comparison with mild virus type HCoV-OC43 equivalent domain demonstrates a unique potential RNA binding pocket alongside the beta-sheet core. Complemented by in vitro binding studies, our data provide several atomic resolution features of SARS-CoV-2 nucleocapsid protein N-terminal domain, guiding the design of novel antiviral agents specific targeting to SARS-CoV-2.\n",
      " \n",
      "Cluster 5.0\n",
      "Title:\n",
      "Rigidity of the Outer Shell Predicted by a Protein Intrinsic Disorder Model Sheds Light on the COVID-19 (Wuhan-2019-nCoV) Infectivity\n",
      "In:\n",
      "Biomolecules\n",
      "Abstract:\n",
      "<p>The world is currently witnessing an outbreak of a new coronavirus spreading quickly across China and affecting at least 24 other countries. With almost 65,000 infected, a worldwide death toll of at least 1370 (as of 14 February 2020), and with the potential to affect up to two-thirds of the world population, COVID-19 is considered by the World Health Organization (WHO) to be a global health emergency. The speed of spread and infectivity of COVID-19 (also known as Wuhan-2019-nCoV) are dramatically exceeding those of the Middle East respiratory syndrome coronavirus (MERS-CoV) and severe acute respiratory syndrome coronavirus (SARS-CoV). In fact, since September 2012, the WHO has been notified of 2494 laboratory-confirmed cases of infection with MERS-CoV, whereas the 2002–2003 epidemic of SARS affected 26 countries and resulted in more than 8000 cases. Therefore, although SARS, MERS, and COVID-19 are all the result of coronaviral infections, the causes of the coronaviruses differ dramatically in their transmissibility. It is likely that these differences in infectivity of coronaviruses can be attributed to the differences in the rigidity of their shells which can be evaluated using computational tools for predicting intrinsic disorder predisposition of the corresponding viral proteins.</p>\n",
      " \n",
      "Cluster 6.0\n",
      "Title:\n",
      "Clinical Features of Patients Infected with the 2019 Novel Coronavirus (COVID-19) in Shanghai, China\n",
      "In:\n",
      "medRxiv\n",
      "Abstract:\n",
      "Background: Since mid-December 2019, a cluster of pneumonia-like diseases caused by a novel coronavirus, now designated COVID-19 by the WHO, emerged in Wuhan city and rapidly spread throughout China. Here we identify the clinical characteristics of COVID-19 in a cohort of patients in Shanghai. Methods: Cases were confirmed by real-time RT-PCR and were analysed for demographic, clinical, laboratory and radiological features. Results: Of 198 patients, the median duration from disease onset to hospital admission was 4 days. The mean age of the patients was 50.1 years, and 51.0% patients were male. The most common symptom was fever. Less than half of the patients presented with respiratory systems including cough, sputum production, itchy or sore throat, shortness of breath, and chest congestion. 5.6% patients had diarrhoea. On admission, T lymphocytes were decreased in 45.8% patients. Ground glass opacity was the most common radiological finding on chest computed tomography. 9.6% were admitted to the ICU because of the development of organ dysfunction. Compared with patients not treated in ICU, patients treated in the ICU were older, had longer waiting time to admission, fever over 38.5o C, dyspnoea, reduced T lymphocytes, elevated neutrophils and organ failure. Conclusions: In this single centre cohort of COVID-19 patients, the most common symptom was fever, and the most common laboratory abnormality was decreased blood T cell counts. Older age, male, fever over 38.5oC, symptoms of dyspnoea, and underlying comorbidity, were the risk factors most associated with severity of disease. Key words: 2019 novel coronavirus; acute respiratory infection; risk factors for disease severity\n",
      "\n",
      "### \n",
      " \n"
     ]
    }
   ],
   "source": [
    "medoidClusterComb = list()\n",
    "for i in range(meanDists.shape[0]):\n",
    "\tfor clustNum in range(numberTopics):\n",
    "\t\tif i in clusters[clustNum]:\n",
    "\t\t\tmedoidClusterComb.append(clustNum)\n",
    "            \n",
    "df_clusters = pd.DataFrame({'cluster':medoidClusterComb}, index=textsIndex)\n",
    "df_combined = df.join(df_clusters, how='outer')\n",
    "\n",
    "medoids_index = [textsIndex[i] for i in medoids]\n",
    "\n",
    "for indx in medoids_index:\n",
    "    print('Cluster ' + str(df_combined['cluster'][indx]))\n",
    "    print('Title:')\n",
    "    print(df_combined['title'][indx])\n",
    "    print('In:')\n",
    "    print(df_combined['journal'][indx])\n",
    "    print('Abstract:')\n",
    "    print(df_combined['abstract_noStmt'][indx])\n",
    "    print(' ')\n",
    "#print([df_all['abstract_noStmt'][i] for i in medoids_lsa])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign word tags to each of the above clusters, we want to find frequent words that are more highly expressed in one cluster compared to the others. For each cluster, we find its distribution across all words and consider only the words that appear more than 100 times in the entire corpus of abstracts. For each of these frequent words we find the mean and standard deviation of the proportion across clusters. We consider a word $i$ a tag for a given cluster $k$ if the frequency of word $i$ in cluster $k$ is at least the mean of word $i$ across clusters plus 1.5 times the standard deviation of word $i$ across all of the clusters. This gives the following tags for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags for Cluster 0\n",
      "['2020', '95', 'basic', 'case', 'china', 'citi', 'confirm', 'control', 'daili', 'data', 'day', 'distribut', 'dynam', 'epidem', 'estim', 'februari', 'hubei', 'incub', 'individu', 'infect', 'infecti', 'interv', 'isol', 'januari', 'mainland', 'march', 'mean', 'model', 'number', 'onset', 'outsid', 'paramet', 'peak', 'peopl', 'period', 'predict', 'provinc', 'quarantin', 'rate', 'report', 'reproduct', 'simul', 'time', 'transmiss', 'would', 'wuhan']\n",
      " \n",
      "Tags for Cluster 1\n",
      "['base', 'indic', 'inform', 'investig', 'method', 'rapid', 'research', 'test']\n",
      " \n",
      "Tags for Cluster 2\n",
      "['2019', 'ace2', 'analysi', 'close', 'coronaviru', 'genom', 'identifi', 'ncov', 'novel', 'pathogen', 'possibl', 'potenti', 'relat', 'sequenc', 'suggest', 'system']\n",
      " \n",
      "Tags for Cluster 3\n",
      "['affect', 'among', 'area', 'assess', 'chang', 'contact', 'contain', 'countri', 'impact', 'implement', 'import', 'intern', 'intervent', 'measur', 'outbreak', 'popul', 'reduc', 'risk', 'spread', 'strategi', 'travel', 'week']\n",
      " \n",
      "Tags for Cluster 4\n",
      "['bind', 'cov', 'design', 'host', 'protein', 'provid', 'rapidli', 'rel', 'rna', 'sars', 'similar', 'structur', 'target']\n",
      " \n",
      "Tags for Cluster 5\n",
      "['detect', 'diagnosi', 'need', 'pcr', 'posit', 'rt', 'sampl', 'signific']\n",
      " \n",
      "Tags for Cluster 6\n",
      "['10', '11', '12', '14', '15', '20', 'admiss', 'age', 'aim', 'characterist', 'clinic', 'common', 'compar', 'count', 'critic', 'ct', 'decreas', 'factor', 'featur', 'fever', 'find', 'group', 'higher', 'hospit', 'ill', 'includ', 'laboratori', 'level', 'lower', 'median', 'medic', 'methods', 'mortal', 'non', 'outcom', 'patient', 'pneumonia', 'results', 'sever', 'significantli', 'symptom', 'vs', 'year']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# tags\n",
    "df_freqs = pd.DataFrame({'word':words, 'total':w.sum(axis=0)})\n",
    "for i in range(numberTopics):\n",
    "\twi = w[clusters[i],]\n",
    "\tdf_freqs['w'+str(i)] = wi.sum(axis=0)/sum(wi.sum(axis=0))\n",
    "df_highfreqs = df_freqs.loc[df_freqs['total']>=100]\n",
    "\n",
    "upper = 1.5*df_highfreqs.drop('total', axis=1).std(axis=1) + df_highfreqs.drop('total', axis=1).mean(axis=1)\n",
    "\n",
    "clusterWords = [[] for i in range(numberTopics)]\n",
    "for i in upper.index:\n",
    "\tif sum(df_highfreqs.drop(['total','word'], axis=1).loc[i,:] >= upper.loc[i]) > 0:\n",
    "\t\thighFreqClust = [j for j, val in enumerate(df_highfreqs.drop(['total','word'], axis=1).loc[i,:] >= upper.loc[i]) if val]\n",
    "\t\tfor c in highFreqClust:\n",
    "\t\t\tclusterWords[c].append(df_highfreqs.loc[i,'word'])\n",
    "            \n",
    "for clust in range(numberTopics):\n",
    "    print('Tags for Cluster ' + str(clust))\n",
    "    print(clusterWords[clust])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words help us to understand what papers in each cluster are about: \n",
    "* Cluster 0 discusses how the virus was spreading, especially intially through China, and epidemiological models.\n",
    "* Cluster 1 appears to discuss research into testing.\n",
    "* Cluster 2 discusses sequencing the COVID-19 genome.\n",
    "* Cluster 3 discusses the spread of the virus.\n",
    "* Cluster 4 discusses the biology of the virus.\n",
    "* Clsuter 5 potentially discusses detection of the virus.\n",
    "* Cluster 6 discusses medical patient care for those diagnosed with COVID-19.\n",
    "\n",
    "Furthermore, these general topics appear to match the medoids of each cluster as given earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Summarization\n",
    "\n",
    "Next we provide a method to summarize each abstract with a with a 1 (or 2) sentence summary. If a given abstract is either one or two sentences, we simply use the entire abstract. If an abstract contains more than 2 sentences, our goal is to determine which sentence best summarizes the entire abstract. That is, we find the sentence that is some sense \"close\" to all the other sentences in the abstract. To do this, we consider each abstract as an individual corpus, and each sentence as an individual document. Similar to the abstract clustering above, we seek to find which document(s) best represent the entire corpus. In the previous section we used LDA and LSA to calculate a distance measure between documents. LDA and LSA, however, do not work well on short documents. Insead, we use a new measure of distance between each of the sentences using only the individual words found within each sentence. We perform the following steps on each corpus (abstract) individually:\n",
    "\n",
    "* Similar to above, we first create a word-document matrix $w$, where $w_{ij}$ gives the number of times word $j$ appears in sentence $i$.\n",
    "* Create a word co-occurrence matrix $c$, where $c_{ij}$ represents the number of times words $i$ and $j$ appear in the same sentence together. $C$ is a symmetric matrix and $c_{ii}$ is the number of sentences in the corpus that contain word $i$.\n",
    "* Create a distance matrix $d$ between words using $c$. We use $$d_{ij} = 2-\\left[P(word~i\\in sentence|word~j\\in sentence) + P(word~j\\in sentence|word_i\\in sentence)\\right]$$ There are many possible distance measure, but the measure above worked well in various experiments of short texts.\n",
    "* Create a distance matrix $s$ between sentences based on the word distance matrix $d$. To calculate the distance between sentences $i$ and $j$, we restrict $D$ such that the columns are words found in sentence $i$, and rows corresponding to words found in sentence $j$. Then let $s_{ij}$ be the mean of this restricted matrix.\n",
    "* A sentence that best summarizes the entire abstract is ideally somewhat related to many of the other sentences in the abstract. For each sentence we calculate the mean distance from that sentence to every other sentence in the abstract. The best summary sentence is chosen as the sentence with the smallest mean distance. \n",
    "\n",
    "The above method returns only one summary sentence per abstract. If we wanted $k$ summary sentences, we can apply k-medoids to the sentence-level distance matrix $s$ and return the $k$ different medoids. We stick to just one summary sentence in this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\robyn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# summary function\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def abstractSummary(entireAbstract):\n",
    "    if len(entireAbstract)==0 or entireAbstract=='' or entireAbstract==' ':\n",
    "    \treturn('No Abstract')\n",
    "    if len(entireAbstract)==0:\n",
    "    \treturn('No Abstract')\n",
    "    abstractCorpus_orig = nltk.tokenize.sent_tokenize(entireAbstract)\n",
    "    abstractCorpus = [preProcessingFcn(sent) for sent in abstractCorpus_orig]\n",
    "    abstractCorpus_orig = [abstractCorpus_orig[i] for i in range(len(abstractCorpus)) if abstractCorpus[i]!='']\n",
    "    abstractCorpus = [sent for sent in abstractCorpus if sent != '']\n",
    "    if(len(abstractCorpus)<=2):\n",
    "        return(entireAbstract)\n",
    "    else:\n",
    "        # word matrix\n",
    "        vectorizer = CountVectorizer(strip_accents='unicode')\n",
    "        textsNew = vectorizer.fit_transform(abstractCorpus)\n",
    "        w = textsNew.toarray()\n",
    "        # word distance matrix\n",
    "        c = w.T.dot(w)\n",
    "        t = np.divide(c, np.diag(c)).transpose()\n",
    "        d = 2 - t - t.T\n",
    "        # sentence distance matrix\n",
    "        n = len(abstractCorpus)\n",
    "        s = np.zeros([n, n])\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i<=j:\n",
    "                    iWordsUse = np.where(w[i,:]>0)[0]\n",
    "                    jWordsUse = np.where(w[j,:]>0)[0]\n",
    "                    dtweetsij = d[np.ix_(iWordsUse, jWordsUse)]\n",
    "                    entries = dtweetsij.shape[0]*dtweetsij.shape[1]\n",
    "                    if entries !=0:\n",
    "                        ij = np.sum(dtweetsij) / entries\n",
    "                        s[i,j] = ij\n",
    "                        s[j,i] = ij\n",
    "        sumDists = s.mean(1)\n",
    "        minDistEntry = np.argmin(sumDists)\n",
    "        summarySentence = abstractCorpus_orig[minDistEntry]\n",
    "        return(summarySentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the summarization algorithm to a random sample of 5 abstracts to demonstrate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire Abstract: \n",
      "Confirmed infection cases in mainland China were analyzed using the data up to January 28, 2020 (first 13 days of reliable confirmed cases). In addition, all available data up to February 3 were processed the same way. For the first period the cumulative number of cases followed an exponential function. However, from January 28, we discerned a downward deviation from the exponential growth. This slower-than-exponential growth was also confirmed by a steady decline of the effective reproduction number. A backtrend analysis suggested the original basic reproduction number R0 to be about 2.4 to 2.5. We used a simple logistic growth model that fitted very well with all data reported until the time of writing . Using this model and the first set of data, we estimate that the maximum cases will be about 21,000 reaching this level in mid-February. Using all available data the maximum number of cases is somewhat higher at 29,000 but its dynamics does not change. These predictions do not account for any possible other secondary sources of infection.\n",
      " \n",
      "Summary:\n",
      "Using all available data the maximum number of cases is somewhat higher at 29,000 but its dynamics does not change.\n",
      " \n",
      " \n",
      "Entire Abstract: \n",
      "We are proposing to use machine learning algorithms to be able to improve possible case identifications of COVID-19 more quicker when we use a mobile phone-based web survey. This will also reduce the spread in the susceptible populations.\n",
      " \n",
      "Summary:\n",
      "We are proposing to use machine learning algorithms to be able to improve possible case identifications of COVID-19 more quicker when we use a mobile phone-based web survey. This will also reduce the spread in the susceptible populations.\n",
      " \n",
      " \n",
      "Entire Abstract: \n",
      "Since December 2019, a newly identified coronavirus (2019 novel coronavirus, 2019-nCov) is causing outbreak of pneumonia in one of largest cities, Wuhan, in Hubei province of China and has draw significant public health attention. The same as severe acute respiratory syndrome coronavirus (SARS-CoV), 2019-nCov enters into host cells via cell receptor angiotensin converting enzyme II (ACE2). In order to dissect the ACE2-expressing cell composition and proportion and explore a potential route of the 2019-nCov infection in digestive system infection, 4 datasets with single-cell transcriptomes of lung, esophagus, gastric, ileum and colon were analyzed. The data showed that ACE2 was not only highly expressed in the lung AT2 cells, esophagus upper and stratified epithelial cells but also in absorptive enterocytes from ileum and colon. These results indicated along with respiratory systems, digestive system is a potential routes for 2019-nCov infection. In conclusion, this study has provided the bioinformatics evidence of the potential route for infection of 2019-nCov in digestive system along with respiratory tract and may have significant impact for our healthy policy setting regards to prevention of 2019-nCoV infection.\n",
      " \n",
      "Summary:\n",
      "These results indicated along with respiratory systems, digestive system is a potential routes for 2019-nCov infection.\n",
      " \n",
      " \n",
      "Entire Abstract: \n",
      "Coronavirus (COVID-19) outbreak in late 2019 and 2020 comprises a serious and more likely a pandemic threat worldwide. Given that the disease has not approved vaccines or drugs up to now, any efforts for drug design and or clinical trails of old drugs based on their mechanism of action are worthy and creditable in such circumstances. Experienced docking experiments using the newly released coordinate structure for COVID-19 protease as a receptor and thoughtfully selected chemicals among antiviral and antibiotics drugs as ligands may be leading in this context. We selected nine drugs from HIV-1 protease inhibitors and twenty-one candidates from anti bronchitis drugs based on their chemical structures and enrolled them in blind and active site-directed dockings in different modes and in native-like conditions of interactions. Our findings suggest the binding capacity and the inhibitory potency of candidates are as follows Tipranavir>Indinavir>Atazanavir>Darunavir>Ritonavir>Amprenavir for HIV-1 protease inhibitors and Cefditoren>Cefixime>Erythromycin>Clarithromycin for anti bronchitis medicines. The drugs bioavailability, their hydrophobicity and the hydrophobic properties of their binding sites and also the rates of their metabolisms and deactivations in the human body are the next determinants for their overall effects on viral infections, the net results that should survey by clinical trials to assess their therapeutic usefulness for coronavirus infections.\n",
      " \n",
      "Summary:\n",
      "We selected nine drugs from HIV-1 protease inhibitors and twenty-one candidates from anti bronchitis drugs based on their chemical structures and enrolled them in blind and active site-directed dockings in different modes and in native-like conditions of interactions.\n",
      " \n",
      " \n",
      "Entire Abstract: \n",
      "A total of 565 Japanese citizens were evacuated from Wuhan, China to Japan. All passengers were screened for symptoms and also undertook reverse transcription polymerase chain reaction testing, identifying 5 asymptomatic and 7 symptomatic passengers testing positive for 2019-nCoV. We show that the screening result is suggestive of the asymptomatic ratio at 41.6%. ### \n",
      " \n",
      "Summary:\n",
      "All passengers were screened for symptoms and also undertook reverse transcription polymerase chain reaction testing, identifying 5 asymptomatic and 7 symptomatic passengers testing positive for 2019-nCoV.\n",
      " \n",
      " \n",
      "Entire Abstract: \n",
      "Radiologic characteristics of 2019 novel coronavirus (2019-nCoV) infected pneumonia (NCIP) which had not been fully understood are especially important for diagnosing and predicting prognosis. We retrospective studied 27 consecutive patients who were confirmed NCIP, the clinical characteristics and CT image findings were collected, and the association of radiologic findings with mortality of patients was evaluated. 27 patients included 12 men and 15 women, with median age of 60 years (IQR 47-69). 17 patients discharged in recovered condition and 10 patients died in hospital. The median age of mortality group was higher compared to survival group (68 (IQR 63-73) vs 55 (IQR 35-60), P = 0.003). The comorbidity rate in mortality group was significantly higher than in survival group (80% vs 29%, P = 0.018). The predominant CT characteristics consisted of ground glass opacity (67%), bilateral sides involved (86%), both peripheral and central distribution (74%), and lower zone involvement (96%). The median CT score of mortality group was higher compared to survival group (30 (IQR 7-13) vs 12 (IQR 11-43), P = 0.021), with more frequency of consolidation (40% vs 6%, P = 0.047) and air bronchogram (60% vs 12%, P = 0.025). An optimal cutoff value of a CT score of 24.5 had a sensitivity of 85.6% and a specificity of 84.5% for the prediction of mortality. 2019-nCoV was more likely to infect elderly people with chronic comorbidities. CT findings of NCIP were featured by predominant ground glass opacities mixed with consolidations, mainly peripheral or combined peripheral and central distributions, bilateral and lower lung zones being mostly involved. A simple CT scoring method was capable to predict mortality.\n",
      "\n",
      "### \n",
      " \n",
      "Summary:\n",
      "The median CT score of mortality group was higher compared to survival group (30 (IQR 7-13) vs 12 (IQR 11-43), P = 0.021), with more frequency of consolidation (40% vs 6%, P = 0.047) and air bronchogram (60% vs 12%, P = 0.025).\n",
      " \n",
      " \n",
      "Entire Abstract: \n",
      "IMPORTANCE: In December 2019, novel coronavirus (2019-nCoV)-infected pneumonia (NCIP) occurred in Wuhan, China. The number of cases has increased rapidly but information on the clinical characteristics of affected patients is limited. OBJECTIVE: To describe the epidemiological and clinical characteristics of NCIP. DESIGN, SETTING, AND PARTICIPANTS: Retrospective, single-center case series of the 138 consecutive hospitalized patients with confirmed NCIP at Zhongnan Hospital of Wuhan University in Wuhan, China, from January 1 to January 28, 2020; final date of follow-up was February 3, 2020. EXPOSURES: Documented NCIP. MAIN OUTCOMES AND MEASURES: Epidemiological, demographic, clinical, laboratory, radiological, and treatment data were collected and analyzed. Outcomes of critically ill patients and noncritically ill patients were compared. Presumed hospital-related transmission was suspected if a cluster of health professionals or hospitalized patients in the same wards became infected and a possible source of infection could be tracked. RESULTS: Of 138 hospitalized patients with NCIP, the median age was 56 years (interquartile range, 42-68; range, 22-92 years) and 75 (54.3%) were men. Hospital-associated transmission was suspected as the presumed mechanism of infection for affected health professionals (40 [29%]) and hospitalized patients (17 [12.3%]). Common symptoms included fever (136 [98.6%]), fatigue (96 [69.6%]), and dry cough (82 [59.4%]). Lymphopenia (lymphocyte count, 0.8 × 109/L [interquartile range {IQR}, 0.6-1.1]) occurred in 97 patients (70.3%), prolonged prothrombin time (13.0 seconds [IQR, 12.3-13.7]) in 80 patients (58%), and elevated lactate dehydrogenase (261 U/L [IQR, 182-403]) in 55 patients (39.9%). Chest computed tomographic scans showed bilateral patchy shadows or ground glass opacity in the lungs of all patients. Most patients received antiviral therapy (oseltamivir, 124 [89.9%]), and many received antibacterial therapy (moxifloxacin, 89 [64.4%]; ceftriaxone, 34 [24.6%]; azithromycin, 25 [18.1%]) and glucocorticoid therapy (62 [44.9%]). Thirty-six patients (26.1%) were transferred to the intensive care unit (ICU) because of complications, including acute respiratory distress syndrome (22 [61.1%]), arrhythmia (16 [44.4%]), and shock (11 [30.6%]). The median time from first symptom to dyspnea was 5.0 days, to hospital admission was 7.0 days, and to ARDS was 8.0 days. Patients treated in the ICU (n = 36), compared with patients not treated in the ICU (n = 102), were older (median age, 66 years vs 51 years), were more likely to have underlying comorbidities (26 [72.2%] vs 38 [37.3%]), and were more likely to have dyspnea (23 [63.9%] vs 20 [19.6%]), and anorexia (24 [66.7%] vs 31 [30.4%]). Of the 36 cases in the ICU, 4 (11.1%) received high-flow oxygen therapy, 15 (41.7%) received noninvasive ventilation, and 17 (47.2%) received invasive ventilation (4 were switched to extracorporeal membrane oxygenation). As of February 3, 47 patients (34.1%) were discharged and 6 died (overall mortality, 4.3%), but the remaining patients are still hospitalized. Among those discharged alive (n = 47), the median hospital stay was 10 days (IQR, 7.0-14.0). CONCLUSIONS AND RELEVANCE: In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41% of patients, 26% of patients received ICU care, and mortality was 4.3%.\n",
      " \n",
      "Summary:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCLUSIONS AND RELEVANCE: In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41% of patients, 26% of patients received ICU care, and mortality was 4.3%.\n",
      " \n",
      " \n",
      "Entire Abstract: \n",
      "There is concern about a new coronavirus, the 2019-nCoV, as a global public health threat. In this article, we provide a preliminary evolutionary and molecular epidemiological analysis of this new virus. A phylogenetic tree has been built using the 15 available whole genome sequence of 2019-nCoV and 12 whole genome sequences highly similar sequences available in gene bank (5 from SARS, 2 from MERS and 5 from Bat SARS-like Coronavirus). FUBAR analysis shows that the Nucleocapsid and the Spike Glycoprotein has some sites under positive pressure while homology modelling helped to explain some molecular and structural differences between the viruses. The phylogenetic tree showed that 2019.nCoV significantly clustered with Bat SARS-like Coronavirus sequence isolated in 2015, whereas structural analysis revealed mutation in S and nucleocapsid proteins. From these results, 2019nCoV could be considered a coronavirus distinct from SARS virus, probably transmitted from bats or another host where mutations conferred upon it the ability to infect humans.\n",
      " \n",
      "Summary:\n",
      "The phylogenetic tree showed that 2019.nCoV significantly clustered with Bat SARS-like Coronavirus sequence isolated in 2015, whereas structural analysis revealed mutation in S and nucleocapsid proteins.\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# apply to sample of abstracts\n",
    "import random\n",
    "\n",
    "random.seed(567)\n",
    "sample = random.sample(range(df.shape[0]), 8)\n",
    "\n",
    "for samp in sample:\n",
    "    print('Entire Abstract: ')\n",
    "    print(df['abstract_noStmt'][samp])\n",
    "    print(' ')\n",
    "    print('Summary:')\n",
    "    print(abstractSummary(df['abstract_noStmt'][samp]))\n",
    "    print(' ')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Breakthroughs\n",
    "\n",
    "Breakthrough in COVID-19 research can take many forms, from breakthroughs in vaccine research to breakthroughs in transmission to breakthoughs in origin. We consider breakthroughs in treatment and prevention of COVID-19 and search for papers that may provide these breakthroughs.\n",
    "\n",
    "We reason that if a paper is a breakthrough in the prevention or cure of the virus, it will contains both words relating breakthrough and words relating to treatments and cures. We create a list of words for both. Note that these lists are not comprehensive and were created by hand. We display abstracts that contain at least two words from each of these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A data-driven drug repositioning framework discovered a potential therapeutic agent targeting COVID-19\n",
      "Abstract: The global spread of SARS-CoV-2 requires an urgent need to find effective therapeutics for the treatment of COVID-19. We developed a data-driven drug repositioning framework, which applies both machine learning and statistical analysis approaches to systematically integrate and mine large-scale knowledge graph, literature and transcriptome data to discover the potential drug candidates against SARS-CoV-2. The retrospective study using the past SARS-CoV and MERS-CoV data demonstrated that our machine learning based method can successfully predict effective drug candidates against a specific coronavirus. Our in silico screening followed by wet-lab validation indicated that a poly-ADP-ribose polymerase 1 (PARP1) inhibitor, CVL218, currently in Phase I clinical trial, may be repurposed to treat COVID-19. Our in vitro assays revealed that CVL218 can exhibit effective inhibitory activity against SARS-CoV-2 replication without obvious cytopathic effect. In addition, we showed that CVL218 is able to suppress the CpG-induced IL-6 production in peripheral blood mononuclear cells, suggesting that it may also have anti-inflammatory effect that is highly relevant to the prevention immunopathology induced by SARS-CoV-2 infection. Further pharmacokinetic and toxicokinetic evaluation in rats and monkeys showed a high concentration of CVL218 in lung and observed no apparent signs of toxicity, indicating the appealing potential of this drug for the treatment of the pneumonia caused by SARS-CoV-2 infection. Moreover, molecular docking simulation suggested that CVL218 may bind to the N-terminal domain of nucleocapsid (N) protein of SARS-CoV-2, providing a possible model to explain its antiviral action. We also proposed several possible mechanisms to explain the antiviral activities of PARP1 inhibitors against SARS-CoV-2, based on the data present in this study and previous evidences reported in the literature. In summary, the PARP1 inhibitor CVL218 discovered by our data-driven drug repositioning framework can serve as a potential therapeutic agent for the treatment of COVID-19.\n",
      "\n",
      "\n",
      "Title: Structure-based drug design, virtual screening and high-throughput screening rapidly identify antiviral leads targeting COVID-19\n",
      "Abstract: A coronavirus identified as 2019 novel coronavirus (COVID-19) is the etiological agent responsible for the 2019-2020 viral pneumonia outbreak that commenced in Wuhan. Currently there is no targeted therapeutics and effective treatment options remain very limited. In order to rapidly discover lead compounds for clinical use, we initiated a program of combined structure-assisted drug design, virtual drug screening, and high-throughput screening to identify new drug leads that target the COVID-19 main protease (Mpro). Mpro is a key coronavirus enzyme, which plays a pivotal role in mediating viral replication and transcription, making it an attractive drug target for this virus. Here, we identified a mechanism-based inhibitor, N3, by computer-aided drug design and subsequently determined the crystal structure of COVID-19 Mpro in complex with this compound. Next, through a combination of structure-based virtual and high-throughput screening, we assayed over 10,000 compounds including approved drugs, drug candidates in clinical trials, and other pharmacologically active compounds as inhibitors of Mpro. Seven of these inhibit Mpro with IC50 values ranging from 0.48 to 16.62 μM. Ebselen, thiadiazolidinone-8 (TDZD-8) and N3 also exhibited strong antiviral activity in cell-based assays. Our results demonstrate the efficacy of this screening strategy, and establishes a new paradigm for the rapid discovery of drug leads with clinical potential in response to new infectious diseases where no specific drugs or vaccines are available.\n",
      "\n",
      "\n",
      "Title: SARS-CoV-2 invades host cells via a novel route: CD147-spike protein\n",
      "Abstract: Currently, COVID-19 caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been widely spread around the world; nevertheless, so far there exist no specific antiviral drugs for treatment of the disease, which poses great challenge to control and contain the virus. Here, we reported a research finding that SARS-CoV-2 invaded host cells via a novel route of CD147-spike protein (SP). SP bound to CD147, a receptor on the host cells, thereby mediating the viral invasion. Our further research confirmed this finding. First, in vitro antiviral tests indicated Meplazumab, an anti-CD147 humanized antibody, significantly inhibited the viruses from invading host cells, with an EC50 of 24.86 μg/mL and IC50 of 15.16 μg/mL. Second, we validated the interaction between CD147 and SP, with an affinity constant of 1.85E-07M. Co-Immunoprecipitation and ELISA also confirmed the binding of the two proteins. Finally, the localization of CD147 and SP was observed in SARS-CoV-2 infected Vero E6 cells by immuno-electron microscope. Therefore, the discovery of the new route CD147-SP for SARS-CoV-2 invading host cells provides a critical target for development of specific antiviral drugs.\n",
      "\n",
      "\n",
      "Title: Effect of large-scale testing platform in prevention and control of the COVID-19 pandemic: an empirical study with a novel numerical model\n",
      "Abstract: Background: China adopted an unprecedented province-scale quarantine since January 23rd 2020, after the novel coronavirus (COVID-19) broke out in Wuhan in December 2019. Responding to the challenge of limited testing capacity, large-scale standardized and fully-automated laboratory (Huo-Yan) was built as an ad-hoc measure. There was so far no empirical data or mathematical model to reveal the impact of the testing capacity improvement since the quarantine. Methods: We integrated public data released by the Health Commission of Hubei Province and Huo-Yan Laboratory testing data into a novel differential model with non-linear transfer coefficients and competitive compartments, to evaluate the trends of suspected cases under different nucleic acid testing capacities. Results: Without the establishment of Huo-Yan, the suspected cases would increased by 47% to 33,700, the corresponding cost of the quarantine would be doubled, and the turning point of the increment of suspected cases and the achievement of \"daily settlement\" (all daily new discovered suspected cases were diagnosed according to the nucleic acid testing results) would be delayed for a whole week and 11 days. If the Huo-Yan Laboratory could ran at its full capacity, the number of suspected cases could started to decrease at least a week earlier, the peak of suspected cases would be reduced by at least 44% and the quarantine cost could be reduced by more than 72%. Ideally, if a daily testing capacity of 10,500 could achieved immediately after the Hubei lockdown, \"daily settlement\" for all suspected cases would be achieved immediately. Conclusions: Large-scale and standardized clinical testing platform with nucleic acid testing, high-throughput sequencing and immunoprotein assessment capabilities need to be implemented simultaneously in order to maximize the effect of quarantine and minimize the duration and cost. Such infrastructure like Huo-Yan, is of great significance for the early prevention and control of infectious diseases for both common times and emergencies.\n",
      "\n",
      "### \n",
      "\n",
      "\n",
      "Title: Network-based Drug Repurposing for Human Coronavirus\n",
      "Abstract: Human Coronaviruses (HCoVs), including severe acute respiratory syndrome coronavirus (SARS-CoV), Middle east respiratory syndrome coronavirus (MERS-CoV), and 2019 novel coronavirus (2019-nCoV), lead global epidemics with high morbidity and mortality. However, there are currently no effective drugs targeting 2019-nCoV. Drug repurposing, represented as an effective drug discovery strategy from existing drugs, could shorten the time and reduce the cost compared to de novo drug discovery. In this study, we present an integrative, antiviral drug repurposing methodology implementing a systems pharmacology-based network medicine platform, quantifying the interplay between the HCoV-host interactome and drug targets in the human protein-protein interaction network. Phylogenetic analyses of 15 HCoV whole genomes reveal that 2019-nCoV has the highest nucleotide sequence identity with SARS-CoV (79.7%) among the six other known pathogenic HCoVs. Specifically, the envelope and nucleocapsid proteins of 2019-nCoV are two evolutionarily conserved regions, having the sequence identities of 96% and 89.6%, respectively, compared to SARS-CoV. Using network proximity analyses of drug targets and known HCoV-host interactions in the human protein-protein interactome, we computationally identified 135 putative repurposable drugs for the potential prevention and treatment of HCoVs. In addition, we prioritized 16 potential anti-HCoV repurposable drugs (including melatonin, mercaptopurine, and sirolimus) that were further validated by enrichment analyses of drug-gene signatures and HCoV-induced transcriptomics data in human cell lines. Finally, we showcased three potential drug combinations (including sirolimus plus dactinomycin, mercaptopurine plus melatonin, and toremifene plus emodin) captured by the Complementary Exposure pattern: the targets of the drugs both hit the HCoV-host subnetwork, but target separate neighborhoods in the human protein-protein interactome network. In summary, this study offers powerful network-based methodologies for rapid identification of candidate repurposable drugs and potential drug combinations toward future clinical trials for HCoVs.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breakthroughWords = ['breakthrough', 'discover', 'discovery', 'develop', 'improve']\n",
    "treatmentWords = ['treament', 'cure', 'vaccine', 'inoculate', 'immunize','prevent', 'drug']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "bkStemmed = [ps.stem(word) for word in breakthroughWords]\n",
    "trStemmed = [ps.stem(word) for word in treatmentWords]\n",
    "\n",
    "for indx in df_combined.index:\n",
    "    if sum([word in df_combined['abstract'][indx] for word in bkStemmed])>1 and sum([word in df_combined['abstract'][indx] for word in trStemmed])>1:\n",
    "        print('Title: ' + df_combined['title'][indx])\n",
    "        print('Abstract: ' +df_combined['abstract_noStmt'][indx])\n",
    "        print('')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022.\n",
    "\n",
    "Dumais, Susan T., et al. \"Using latent semantic analysis to improve access to textual information.\" Proceedings of the SIGCHI conference on Human factors in computing systems. 1988."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
